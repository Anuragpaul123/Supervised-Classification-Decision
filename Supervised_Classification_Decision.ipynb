{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "->Information Gain (IG) is a key concept in machine learning, particularly in Decision Tree algorithms. It measures the reduction in uncertainty or entropy achieved by splitting a dataset based on a specific feature.\n",
        "\n",
        "Information Gain is calculated as the difference between the Entropy of the parent node (before the split) and the weighted average Entropy of the child nodes (after the split)\n",
        "\n",
        "2.What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "->Gini Impurity and Entropy are both mathematical measures of impurity or disorder in a set of data, and they are the two most common criteria used by Decision Tree algorithms (like CART, ID3, and C4.5) to decide the optimal split point at any given node.\n",
        "\n",
        "3. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "->Pre-Pruning in Decision Trees, also known as Early Stopping, is a technique used to prevent the decision tree from growing too large during the training phase itself, thereby mitigating the risk of overfitting.\n",
        "\n",
        "Unlike post-pruning, which involves cutting back a fully grown tree, pre-pruning imposes constraints that stop the tree-building process before a node is split, turning the node into a leaf if the splitting criteria are not met.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dcu-TninTyK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train Decision Tree using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Feature importances\n",
        "importances = pd.Series(clf.feature_importances_, index=iris.feature_names)\n",
        "print(\"Feature Importances:\")\n",
        "print(importances.sort_values(ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UibvSbD5Uo0M",
        "outputId": "25e421e5-63ad-4ab2-a432-546557cf9cda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "petal length (cm)    0.564056\n",
            "petal width (cm)     0.422611\n",
            "sepal length (cm)    0.013333\n",
            "sepal width (cm)     0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "->A Support Vector Machine (SVM) is a powerful and flexible supervised machine learning algorithm primarily used for classification (though it can also be used for regression).\n",
        "\n",
        "Its core principle is to find the optimal hyperplane that distinctly separates the data points of different classes in an N-dimensional space. The \"optimal\" hyperplane is the one that achieves the maximum margin between the classes, which leads to better generalization on unseen data.\n",
        "\n",
        "6.What is the Kernel Trick in SVM?\n",
        "\n",
        "->The Kernel Trick is a fundamental mathematical concept in Support Vector Machines (SVMs) that allows the algorithm to effectively classify non-linearly separable data without explicitly transforming the data into a high-dimensional space.\n",
        "\n",
        "The \"trick\" is that the SVM's optimization problem relies only on the dot product (inner product) between data points. The kernel function is designed to compute this dot product implicitly in a higher-dimensional feature space, which is computationally much cheaper than actually calculating the coordinates of every single data point in that new space\n"
      ],
      "metadata": {
        "id": "6rWCPRtuU3YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {linear_acc:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy: {rbf_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPEWz-1vVMU_",
        "outputId": "5d5a91ef-2d52-4ca2-bfbd-2f06ea76cd49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0000\n",
            "RBF Kernel Accuracy: 0.8056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "->The Naïve Bayes classifier is a family of simple, yet effective, probabilistic classifiers used for classification tasks (like spam detection, sentiment analysis, and text classification). It is a supervised machine learning algorithm based on Bayes' Theorem.\n",
        "\n",
        "The classifier works by calculating the probability of a given sample belonging to a particular class (the posterior probability) based on the combined probabilities of its individual features. It then predicts the class that has the maximum posterior probability.\n",
        "\n",
        "9.  Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "->The different variants of the Naïve Bayes classifier—Gaussian, Multinomial, and Bernoulli—are distinguished by the assumptions they make about the probability distribution of the features ($P(x_i|C)$) and, consequently, the type of data they are designed to handle.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7cBtXrWHVWTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10 Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"GaussianNB Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3_fpLt5Vsui",
        "outputId": "f77b3cf0-fe3d-40bb-9323-0c2aa64887e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GaussianNB Accuracy: 0.9737\n"
          ]
        }
      ]
    }
  ]
}